#!/usr/bin/env python
# coding: utf-8

# In[43]:


import json
import os
import pandas as pd

final_df=pd.DataFrame(columns=['Class'])

def file_parse(entry):
    #print(entry)
    #Parsing through all the json files
    f = open(entry)
    temp_dict={}
    global final_df
    #Assigning a class based on file name
    if("BENIGN") in entry:
        temp_dict['Class'] = 0
    else:
        temp_dict['Class'] = 1
    temp_dict['file_name']=entry
    data=json.load(f)
    for i in data['behavior']['apistats']:
        #Looping through all the subkeys and summing the repeated API calls into a dictionary
        for key,value in data['behavior']['apistats'][i].items():
            if key in temp_dict:
                temp_dict[key]=temp_dict[key] + data['behavior']['apistats'][i][key]
            else:
                temp_dict[key]=value
     #Converting dictionary to dataframe           
    data= pd.DataFrame([temp_dict])
    #Concatenating all the dictionary values to a dataframe
    final_df= pd.concat([final_df, data], ignore_index=True, sort=False,join='outer') 
    f.close()

#Parsing through each file in directory
for entry in os.scandir():
    if (entry.path.endswith(".json")
            or entry.path.endswith(".json")) and entry.is_file():
        #print(entry.path)
        file_parse(entry.path)


        


# In[53]:


#Filtering out the columns containing values less than threshold percentage
perc= 50
min_count=int((perc/100)*final_df.shape[0]+1)
new_df=final_df.dropna(axis=1,thresh=min_count)
#filling all the n/a fields with zero
one_df = new_df.drop(['file_name'],axis=1).fillna(0)
one_df


# In[54]:


#Separating training features and labels
y=one_df['Class']
X=one_df.drop(['Class'],axis=1)


# In[55]:


#Splitting into traininig and testing dataset
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)


# In[56]:


#Scaling features into same scale
import numpy as np
from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
X_train_scaled=ss.fit_transform(X_train)
X_test_scaled=ss.transform(X_test)
y_train=np.array(y_train)


# In[57]:


#training random forest classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model = RandomForestClassifier(random_state=1)
model.fit(X_train,y_train)
predic_y=model.predict(X_test)
acc_random_forest=accuracy_score(y_test,predic_y)


# In[58]:


#Training support vector machine
from sklearn import svm
svm_clf=svm.SVC(kernel='linear')
svm_clf.fit(X_train,y_train)
y_pred_svm=svm_clf.predict(X_test)
acc_SVM=accuracy_score(y_test,y_pred_svm)


# In[59]:


print('The accuracy of SVM model trained with columns having api features in more than '+str(100-perc)+'% files is '+str(acc_SVM))
print('The accuracy of RF model trained with columns having api features in more than '+str(100-perc)+'% files is '+ str(acc_random_forest))

